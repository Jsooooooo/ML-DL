{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QA_Machine.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1P5Azw3X4nj9pemy0ee7npXrvMPxfW1X0",
      "authorship_tag": "ABX9TyMSRyol5K0leFBG4a9i1KZ5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jsooooooo/ML-DL/blob/master/QA_Machine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOOORz8ySZJn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.jit import script, trace\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import csv\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "import unicodedata\n",
        "import codecs\n",
        "from io import open\n",
        "import itertools\n",
        "import math\n",
        "\n",
        "# 判断GPU还是CPU\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmf3hCeL39In",
        "colab_type": "text"
      },
      "source": [
        "/content/drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zRsO1DHjYDs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "81f73432-a4f3-41f5-b74f-70cd2dd7a131"
      },
      "source": [
        "# 装载Google云盘到当前会话空间，那么云盘中的文件就在 /driver 目录下\n",
        "from google.colab import drive\n",
        "drive.mount('/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RojSnihFbYht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 日志输出\n",
        "def log(method,a,b):\n",
        "  print(\"-------------------------\")\n",
        "  print(method,\":\")\n",
        "  print(a,\":\",b)\n",
        "  print(\"-------------------------\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Sp4EK6TcRyy",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# 1.格式化数据\n",
        "加载数据集，是一个电影角色之间的对话\n",
        "数据集是一些txt文件，首先要处理变为csv文件方便读取\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rP9fMx7QXpS0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 文件分割处理\n",
        "\n",
        "# 将文件的每一行拆分为字段字典，key为ID，value为行\n",
        "def loadLines(fileName, fields):\n",
        "  lines = {}\n",
        "  with open(fileName, 'r', encoding='iso-8859-1') as f:\n",
        "    for line in f:\n",
        "      values = line.split(\" +++$+++ \")\n",
        "\n",
        "      # Extract fields\n",
        "      lineObj = {}\n",
        "      for i, field in enumerate(fields):\n",
        "        lineObj[field] = values[i]\n",
        "      lines[lineObj['lineID']] = lineObj\n",
        "  return lines\n",
        "\n",
        "\n",
        "# 将 `loadLines` 中的行字段分组为基于 *movie_conversations.txt* 的对话\n",
        "def loadConversations(fileName, lines, fields):\n",
        "  conversations = []\n",
        "  with open(fileName, 'r', encoding='iso-8859-1') as f:\n",
        "   for line in f:\n",
        "     values = line.split(\" +++$+++ \")\n",
        "\n",
        "     # Extract fields\n",
        "     convObj = {}\n",
        "     for i, field in enumerate(fields):\n",
        "       convObj[field] = values[i]\n",
        "\n",
        "     # Convert string to list (convObj[\"utteranceIDs\"] == \"['L598485', 'L598486', ...]\")\n",
        "     lineIds = eval(convObj[\"utteranceIDs\"])\n",
        "\n",
        "     # Reassemble lines\n",
        "     convObj[\"lines\"] = []\n",
        "     for lineId in lineIds:\n",
        "       convObj[\"lines\"].append(lines[lineId])\n",
        "     conversations.append(convObj)\n",
        "  return conversations\n",
        "\n",
        "\n",
        "# 从对话中提取一对句子\n",
        "def extractSentencePairs(conversations):\n",
        "  qa_pairs = []\n",
        "  for conversation in conversations:\n",
        "   # Iterate over all the lines of the conversation\n",
        "   for i in range(len(conversation[\"lines\"]) - 1): # We ignore the last line(no answer for it)\n",
        "     inputLine = conversation[\"lines\"][i][\"text\"].strip()\n",
        "     targetLine = conversation[\"lines\"][i+1][\"text\"].strip()\n",
        "     # Filter wrong samples (if one of the lists is empty)\n",
        "     if inputLine and targetLine:\n",
        "       qa_pairs.append([inputLine, targetLine])\n",
        "  return qa_pairs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDkYhyGCfi3k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "outputId": "4b62484a-6ba0-4a29-89b1-63cc1a6841c4"
      },
      "source": [
        "# 将处理好的写入新txt文件，再写入csv\n",
        "\n",
        "# 源文件路径\n",
        "corpus_name = \"cornell movie-dialogs corpus\"\n",
        "corpus = os.path.join(\"/drive/My Drive/Dataset\", corpus_name)\n",
        "\n",
        "# 定义新文件的路径\n",
        "datafile = os.path.join(corpus, \"formatted_movie_lines.txt\")\n",
        "delimiter = '\\t'\n",
        "delimiter = str(codecs.decode(delimiter, \"unicode_escape\"))\n",
        "\n",
        "# 初始化行dict，对话列表和字段ID\n",
        "lines = {}\n",
        "conversations = []\n",
        "MOVIE_LINES_FIELDS = [\"lineID\", \"characterID\", \"movieID\", \"character\", \"text\"]\n",
        "MOVIE_CONVERSATIONS_FIELDS = [\"character1ID\", \"character2ID\", \"movieID\", \"utteranceIDs\"]\n",
        "\n",
        "# 加载行和进程对话\n",
        "print(\"\\nProcessing corpus...\")\n",
        "lines = loadLines(os.path.join(corpus, \"movie_lines.txt\"), MOVIE_LINES_FIELDS)\n",
        "print(\"\\nLoading conversations...\")\n",
        "conversations = loadConversations(os.path.join(corpus, \"movie_conversations.txt\"),lines, MOVIE_CONVERSATIONS_FIELDS)\n",
        "\n",
        "# 写入新的csv文件\n",
        "print(\"\\nWriting newly formatted file...\")\n",
        "with open(datafile, 'w', encoding='utf-8') as outputfile:\n",
        "  writer = csv.writer(outputfile, delimiter=delimiter, lineterminator='\\n')\n",
        "  for pair in extractSentencePairs(conversations):\n",
        "    writer.writerow(pair)\n",
        "# 打印一个样本的行\n",
        "print(\"\\nSample lines from file:\")\n",
        "\n",
        "def printLines(file, n=10):\n",
        "  with open(file, 'rb') as datafile:\n",
        "    lines = datafile.readlines()\n",
        "  for line in lines[:n]:\n",
        "    print(line)\n",
        "printLines(datafile)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Processing corpus...\n",
            "\n",
            "Loading conversations...\n",
            "\n",
            "Writing newly formatted file...\n",
            "\n",
            "Sample lines from file:\n",
            "b\"Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\\tWell, I thought we'd start with pronunciation, if that's okay with you.\\n\"\n",
            "b\"Well, I thought we'd start with pronunciation, if that's okay with you.\\tNot the hacking and gagging and spitting part.  Please.\\n\"\n",
            "b\"Not the hacking and gagging and spitting part.  Please.\\tOkay... then how 'bout we try out some French cuisine.  Saturday?  Night?\\n\"\n",
            "b\"You're asking me out.  That's so cute. What's your name again?\\tForget it.\\n\"\n",
            "b\"No, no, it's my fault -- we didn't have a proper introduction ---\\tCameron.\\n\"\n",
            "b\"Cameron.\\tThe thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\\n\"\n",
            "b\"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\\tSeems like she could get a date easy enough...\\n\"\n",
            "b'Why?\\tUnsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\\n'\n",
            "b\"Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\\tThat's a shame.\\n\"\n",
            "b'Gosh, if only we could find Kat a boyfriend...\\tLet me see what I can do.\\n'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJ0cgDR4mYkb",
        "colab_type": "text"
      },
      "source": [
        "# 2.加载和清洗数据\n",
        "1.   加载\n",
        "  1.   首先创建词汇表（Vocabulary），并将这些句子转换为句子对\n",
        "  2.   创建一个Voc类，存储单词到索引的映射，索引到单词反映射，每个句子单词计数和总单词量\n",
        "  3.   Voc类方法：添加单个单词、添加句子中所有单词、清洗计数低于阈值的单词\n",
        "2.   清洗\n",
        "  1.   将unicode转为ASCII码\n",
        "  2.   将所有字母转为小写\n",
        "  3.   清洗除基本标点之外的字符\n",
        "  4.   过滤超出最大长度的句子\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6QoOwYViYdM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 默认词向量\n",
        "PAD_token = 0 # 用来填充短句子\n",
        "SOS_token = 1 # Start-of-sentence 标记\n",
        "EOS_token = 2 # End-of-sentence 标记\n",
        "\n",
        "class Voc:\n",
        "  def __init__(self, name):\n",
        "    # 单词表名字\n",
        "    self.name = name\n",
        "    # 是否已修建标识\n",
        "    self.trimmed = False\n",
        "    # 单词到索引的映射字典 {单词:当前位置索引}\n",
        "    self.word2index = {}\n",
        "    # 某个单词的数目字典  {单词:出现次数}\n",
        "    self.word2count = {}\n",
        "    # 索引到单词的映射字典 {当前位置索引:单词}\n",
        "    self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "    # 单词表的大小（当前位置）\n",
        "    self.num_words = 3 # Count SOS, EOS, PAD\n",
        "  \n",
        "  # 添加句子中的所有单词，先分割\n",
        "  def addSentence(self, sentence):\n",
        "    for word in sentence.split(' '):\n",
        "      self.addWord(word)\n",
        "  \n",
        "  # 添加单个单词到单词表\n",
        "  def addWord(self, word):\n",
        "    if word not in self.word2index:\n",
        "      self.word2index[word] = self.num_words\n",
        "      self.word2count[word] = 1\n",
        "      self.index2word[self.num_words] = word\n",
        "      self.num_words += 1\n",
        "    else:\n",
        "      self.word2count[word] += 1\n",
        "  \n",
        "  # 删除低于特定计数阈值的单词\n",
        "  def trim(self, min_count):\n",
        "    # 排除已经修剪过的\n",
        "    if self.trimmed:\n",
        "      return\n",
        "    self.trimmed = True\n",
        "    \n",
        "    keep_words = []\n",
        "    for k, v in self.word2count.items():\n",
        "      if v >= min_count:\n",
        "        keep_words.append(k)\n",
        "      print('keep_words {} / {} = {:.4f}'.format(\n",
        "      len(keep_words), len(self.word2index), len(keep_words) /\n",
        "      len(self.word2index)\n",
        "      ))\n",
        "    \n",
        "    # 重初始化字典\n",
        "    self.word2index = {}\n",
        "    self.word2count = {}\n",
        "    self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "    self.num_words = 3 # Count default tokens\n",
        "    \n",
        "    # 修剪完，清空所有映射数组，重新讲这些修剪后的单词加入单词表\n",
        "    for word in keep_words:\n",
        "      self.addWord(word)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2dK0aThMCqB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "bf47db43-7360-4338-d079-26f56da0c92f"
      },
      "source": [
        "def unicodeToAscii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "  if unicodedata.category(c) != 'Mn'\n",
        "  )\n",
        "\n",
        "ii = Voc(\"voc\")\n",
        "ii.addSentence(unicodeToAscii(\"I want new balance\"))\n",
        "ii.addSentence(unicodeToAscii(\"no new balance doesn't fit me\"))\n",
        "print (ii.word2index)\n",
        "print (ii.word2count)\n",
        "print (ii.index2word)\n",
        "print (ii.num_words)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'I': 3, 'want': 4, 'new': 5, 'balance': 6, 'no': 7, \"doesn't\": 8, 'fit': 9, 'me': 10}\n",
            "{'I': 1, 'want': 1, 'new': 2, 'balance': 2, 'no': 1, \"doesn't\": 1, 'fit': 1, 'me': 1}\n",
            "{0: 'PAD', 1: 'SOS', 2: 'EOS', 3: 'I', 4: 'want', 5: 'new', 6: 'balance', 7: 'no', 8: \"doesn't\", 9: 'fit', 10: 'me'}\n",
            "11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odMamFd4s9nf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "585e773c-2603-44b8-f39e-688c540fa48e"
      },
      "source": [
        "MAX_LENGTH = 10 # Maximum sentence length to consider\n",
        "\n",
        "# 将Unicode字符串转换为纯ASCII，多亏了\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "  if unicodedata.category(c) != 'Mn'\n",
        "  )\n",
        "\n",
        "\n",
        "# 初始化Voc对象，并把格式化pairs对话存放到list中\n",
        "def readVocs(datafile, corpus_name):\n",
        "  print(\"Reading lines...\")\n",
        "  # 读取文件，并按行分割\n",
        "  lines = open(datafile, encoding='utf-8').read().strip().split('\\n')\n",
        "\n",
        "  # 每行都是以 \\t 分割句子对，将每个句子中的单词加入单词表\n",
        "  pairs = [[unicodeToAscii(s) for s in l.split('\\t')] for l in lines]\n",
        "  # 创建一个单词表对象，命名为数据集的名字\n",
        "  voc = Voc(corpus_name)\n",
        "  return voc, pairs\n",
        "\n",
        "\n",
        "# 如果对 'p' 中（一行数据，即一个句子对）的两个句子都低于 MAX_LENGTH 阈值，则返回True\n",
        "def filterPair(p):\n",
        "  # Input sequences need to preserve the last word for EOS token\n",
        "  return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n",
        "\n",
        "\n",
        "# 过滤满足条件的 pairs 对话（对话即一行，两个句子）\n",
        "def filterPairs(pairs):\n",
        "  return [pair for pair in pairs if filterPair(pair)]\n",
        "\n",
        "\n",
        "# 使用上面定义的函数，返回一个填充的voc对象和对列表\n",
        "def loadPrepareData(corpus, corpus_name, datafile, save_dir):\n",
        "  print(\"Start preparing training data ...\")\n",
        "  voc, pairs = readVocs(datafile, corpus_name)\n",
        "  print(\"读取 {!s} 个句子对\".format(len(pairs)))\n",
        "  pairs = filterPairs(pairs)\n",
        "  print(\"清洗后剩余 {!s} 个句子对\".format(len(pairs)))\n",
        "  print(\"Counting words...\")\n",
        "  for pair in pairs:\n",
        "    voc.addSentence(pair[0])\n",
        "    voc.addSentence(pair[1])\n",
        "  print(\"Counted words:\", voc.num_words)\n",
        "  return voc, pairs\n",
        "\n",
        "\n",
        "# 加载voc和句子对（voc、pairs）\n",
        "save_dir = os.path.join(\"data\", \"save\")\n",
        "voc, pairs = loadPrepareData(corpus, corpus_name, datafile, save_dir)\n",
        "# 打印一些对进行验证\n",
        "print(\"\\npairs:\")\n",
        "for pair in pairs[:10]:\n",
        "  print(pair)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start preparing training data ...\n",
            "Reading lines...\n",
            "读取 221282 个句子对\n",
            "清洗后剩余 88265 个句子对\n",
            "Counting words...\n",
            "Counted words: 61758\n",
            "\n",
            "pairs:\n",
            "['Gosh, if only we could find Kat a boyfriend...', 'Let me see what I can do.']\n",
            "[\"C'esc ma tete. This is my head\", \"Right.  See?  You're ready for the quiz.\"]\n",
            "[\"That's because it's such a nice one.\", 'Forget French.']\n",
            "['There.', 'Where?']\n",
            "['You have my word.  As a gentleman', \"You're sweet.\"]\n",
            "['Hi.', 'Looks like things worked out tonight, huh?']\n",
            "['You know Chastity?', 'I believe we share an art instructor']\n",
            "['Have fun tonight?', 'Tons']\n",
            "['Well, no...', \"Then that's all you had to say.\"]\n",
            "[\"Then that's all you had to say.\", 'But']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkaWOrfcErgr",
        "colab_type": "text"
      },
      "source": [
        "# 3.词嵌入-为模型提供矩阵数据\n",
        "1.   将数据转化为torch的tensor形式\n",
        "2.   seq2seq模型中的batch_size设为1，说明是把\n",
        "3.   使用GPU并行运算，需用mini_batch\n",
        "4.   但是mini_batch每个句子长短不同，所以需将短句子进行补0填充\n",
        "5.   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nukmIrddW3Ui",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5504e3ee-d9a1-4b02-8ccf-5754b58f1c6b"
      },
      "source": [
        "# 返回一个句子中单词所对应的索引组成的列表\n",
        "def indexesFromSentence(voc, sentence):\n",
        "  # log(\"indexesFromSentence\",\"sentence\",sentence)\n",
        "  return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n",
        "\n",
        "print (indexesFromSentence(ii,\"I no want balance\"))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3, 7, 4, 6, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqOhhRNSDInx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "73a39455-9148-43bd-e44f-cb4887e7dfd7"
      },
      "source": [
        "# 返回一个句子中单词所对应的索引组成的列表\n",
        "def indexesFromSentence(voc, sentence):\n",
        "  log(\"indexesFromSentence\",\"sentence\",sentence)\n",
        "  return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n",
        "\n",
        "\n",
        "# zip_longest 对数据进行转置，并补0操作\n",
        "def zeroPadding(l, fillvalue=PAD_token):\n",
        "  # zip_longest:将元素行转置成一列，并在末尾补0\n",
        "  # 单个星号接受多个非字典值形成一个列表\n",
        "  log(\"zeroPadding\",\"l\",l)\n",
        "  log(\"zeroPadding\",\"l padding\",list(itertools.zip_longest(*l, fillvalue=fillvalue)))\n",
        "  return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
        "\n",
        "\n",
        "# 用一个矩阵m 记录PAD_token填充的的位置为0，其他的为1\n",
        "def binaryMatrix(l, value=PAD_token):\n",
        "  m = []\n",
        "  for i, seq in enumerate(l):\n",
        "    m.append([])\n",
        "    for token in seq:\n",
        "      if token == PAD_token:\n",
        "        m[i].append(0)\n",
        "      else:\n",
        "        m[i].append(1)\n",
        "  return m\n",
        "\n",
        "\n",
        "# 返回填充前句子对的前一句（即问题句）的长度和填充后的输入序列张量\n",
        "def inputVar(l, voc):\n",
        "  indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
        "  lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "  padList = zeroPadding(indexes_batch)\n",
        "  padVar = torch.LongTensor(padList)\n",
        "  return padVar, lengths\n",
        "\n",
        "\n",
        "# 返回填充前句子对的后一句（即回答句）最长的一个长度和填充后的输入序列张量,和填充后的标记矩阵mask\n",
        "def outputVar(l, voc):\n",
        "  indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
        "  max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
        "  padList = zeroPadding(indexes_batch)\n",
        "  mask = binaryMatrix(padList)\n",
        "  mask = torch.ByteTensor(mask)\n",
        "  padVar = torch.LongTensor(padList)\n",
        "  return padVar, mask, max_target_len\n",
        "\n",
        "\n",
        "# 返回给定batch对的所有项目\n",
        "def batch2TrainData(voc, pair_batch):\n",
        "  pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
        "  input_batch, output_batch = [], []\n",
        "  for pair in pair_batch:\n",
        "    input_batch.append(pair[0])\n",
        "    output_batch.append(pair[1])\n",
        "  inp, lengths = inputVar(input_batch, voc)\n",
        "  output, mask, max_target_len = outputVar(output_batch, voc)\n",
        "  return inp, lengths, output, mask, max_target_len\n",
        "\n",
        "\n",
        "# 验证例子\n",
        "small_batch_size = 5\n",
        "batches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])\n",
        "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
        "print(\"input_variable:\", input_variable)\n",
        "print(\"lengths:\", lengths)\n",
        "print(\"target_variable:\", target_variable)\n",
        "print(\"mask:\", mask)\n",
        "print(\"max_target_len:\", max_target_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------------\n",
            "indexesFromSentence :\n",
            "sentence : I am the Lord's voice in this town.\n",
            "-------------------------\n",
            "-------------------------\n",
            "indexesFromSentence :\n",
            "sentence : And still Mrs. Harold Ryan?\n",
            "-------------------------\n",
            "-------------------------\n",
            "indexesFromSentence :\n",
            "sentence : I like you, Dil --\n",
            "-------------------------\n",
            "-------------------------\n",
            "indexesFromSentence :\n",
            "sentence : It's 350 pages long.\n",
            "-------------------------\n",
            "-------------------------\n",
            "indexesFromSentence :\n",
            "sentence : Where is your mother?\n",
            "-------------------------\n",
            "-------------------------\n",
            "zeroPadding :\n",
            "l : [[16, 758, 32, 8209, 5615, 140, 81, 6183, 2], [228, 676, 1439, 10573, 14799, 2], [16, 52, 906, 37950, 216, 2], [146, 25496, 25497, 7570, 2], [122, 23, 114, 3718, 2]]\n",
            "-------------------------\n",
            "-------------------------\n",
            "zeroPadding :\n",
            "l padding : [(16, 228, 16, 146, 122), (758, 676, 52, 25496, 23), (32, 1439, 906, 25497, 114), (8209, 10573, 37950, 7570, 3718), (5615, 14799, 216, 2, 2), (140, 2, 2, 0, 0), (81, 0, 0, 0, 0), (6183, 0, 0, 0, 0), (2, 0, 0, 0, 0)]\n",
            "-------------------------\n",
            "-------------------------\n",
            "indexesFromSentence :\n",
            "sentence : Wallowing in sin and lust...\n",
            "-------------------------\n",
            "-------------------------\n",
            "indexesFromSentence :\n",
            "sentence : Will you please go?  An emergency!\n",
            "-------------------------\n",
            "-------------------------\n",
            "indexesFromSentence :\n",
            "sentence : Give me a bit more, baby, a bit more.\n",
            "-------------------------\n",
            "-------------------------\n",
            "indexesFromSentence :\n",
            "sentence : Yeah, but the margins are real wide.\n",
            "-------------------------\n",
            "-------------------------\n",
            "indexesFromSentence :\n",
            "sentence : Stepmother.  She's out torturing the movers.\n",
            "-------------------------\n",
            "-------------------------\n",
            "zeroPadding :\n",
            "l : [[54855, 140, 32476, 459, 54856, 2], [2346, 74, 3017, 125, 27, 469, 14805, 2], [876, 13, 10, 3930, 7508, 8992, 10, 3930, 3929, 2], [317, 259, 32, 25498, 173, 1523, 25499, 2], [35629, 27, 387, 55, 35630, 32, 35631, 2]]\n",
            "-------------------------\n",
            "-------------------------\n",
            "zeroPadding :\n",
            "l padding : [(54855, 2346, 876, 317, 35629), (140, 74, 13, 259, 27), (32476, 3017, 10, 32, 387), (459, 125, 3930, 25498, 55), (54856, 27, 7508, 173, 35630), (2, 469, 8992, 1523, 32), (0, 14805, 10, 25499, 35631), (0, 2, 3930, 2, 2), (0, 0, 3929, 0, 0), (0, 0, 2, 0, 0)]\n",
            "-------------------------\n",
            "input_variable: tensor([[   16,   228,    16,   146,   122],\n",
            "        [  758,   676,    52, 25496,    23],\n",
            "        [   32,  1439,   906, 25497,   114],\n",
            "        [ 8209, 10573, 37950,  7570,  3718],\n",
            "        [ 5615, 14799,   216,     2,     2],\n",
            "        [  140,     2,     2,     0,     0],\n",
            "        [   81,     0,     0,     0,     0],\n",
            "        [ 6183,     0,     0,     0,     0],\n",
            "        [    2,     0,     0,     0,     0]])\n",
            "lengths: tensor([9, 6, 6, 5, 5])\n",
            "target_variable: tensor([[54855,  2346,   876,   317, 35629],\n",
            "        [  140,    74,    13,   259,    27],\n",
            "        [32476,  3017,    10,    32,   387],\n",
            "        [  459,   125,  3930, 25498,    55],\n",
            "        [54856,    27,  7508,   173, 35630],\n",
            "        [    2,   469,  8992,  1523,    32],\n",
            "        [    0, 14805,    10, 25499, 35631],\n",
            "        [    0,     2,  3930,     2,     2],\n",
            "        [    0,     0,  3929,     0,     0],\n",
            "        [    0,     0,     2,     0,     0]])\n",
            "mask: tensor([[1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1],\n",
            "        [0, 1, 1, 1, 1],\n",
            "        [0, 1, 1, 1, 1],\n",
            "        [0, 0, 1, 0, 0],\n",
            "        [0, 0, 1, 0, 0]], dtype=torch.uint8)\n",
            "max_target_len: 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xr4LoSSK82i6",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "zip_longest函数用法\n",
        "```\n",
        "res = zip_longest('abc', '12')\n",
        "for x in res:\n",
        "    print(x)\n",
        "```\n",
        "\n",
        "结果\n",
        "```\n",
        "('a', '1')\n",
        "('b', '2')\n",
        "('c', None)\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBRGORRfJwRg",
        "colab_type": "text"
      },
      "source": [
        "# 4.定义Seq2Seq模型\n",
        "1.   首先将上述单词索引经过处理得到的tensor数据进行词嵌入\n",
        "2.   为RNN模块打包填充batch序列。\n",
        "3.   通过GRU进行前向传播。\n",
        "4.   反填充。\n",
        "5.   对双向GRU输出求和。\n",
        "6.   返回输出和最终隐藏状态。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FdotNOfJqsy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 编码器GRU\n",
        "\n",
        "class EncoderRNN(nn.Module):\n",
        "  def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
        "    super(EncoderRNN, self).__init__()\n",
        "    self.n_layers = n_layers\n",
        "    self.hidden_size = hidden_size\n",
        "    self.embedding = embedding\n",
        "\n",
        "    # 初始化GRU; input_size和hidden_size参数都设置为'hidden_size'\n",
        "    # 因为我们的输入大小是一个嵌入了多个特征的单词== hidden_size\n",
        "    self.gru = nn.GRU(hidden_size, hidden_size, n_layers,dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
        " \n",
        " \n",
        "\n",
        "  def forward(self, input_seq, input_lengths, hidden=None):\n",
        "    # 将单词索引转换为词向量\n",
        "    embedded = self.embedding(input_seq)\n",
        "    # 为RNN模块打包填充batch序列\n",
        "    packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
        "    # 正向通过GRU\n",
        "    outputs, hidden = self.gru(packed, hidden)\n",
        "    # 打开填充\n",
        "    outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
        "    # 总和双向GRU输出\n",
        "    outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
        "    # 返回输出和最终隐藏状态\n",
        "    return outputs, hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMhU1wGNMXDy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 注意力子模块\n",
        "# Luong的attention layer\n",
        "class Attn(torch.nn.Module):\n",
        "  def __init__(self, method, hidden_size):\n",
        "    super(Attn, self).__init__()\n",
        "    \n",
        "    self.method = method\n",
        "    \n",
        "\n",
        "    # 判断注意力计算方式\n",
        "    if self.method not in ['dot', 'general', 'concat']:\n",
        "      raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
        "    \n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    # 初始化attention，使用一个小神经网络自主学习\n",
        "    if self.method == 'general':\n",
        "      self.attn = torch.nn.Linear(self.hidden_size, hidden_size)\n",
        "    elif self.method == 'concat':\n",
        "      self.attn = torch.nn.Linear(self.hidden_size * 2, hidden_size)\n",
        "      self.v = torch.nn.Parameter(torch.FloatTensor(hidden_size))\n",
        "\n",
        "\n",
        "  # 三种计算注意力能量的方式\n",
        "  def dot_score(self, hidden, encoder_output):\n",
        "    return torch.sum(hidden * encoder_output, dim=2)\n",
        "\n",
        "  def general_score(self, hidden, encoder_output):\n",
        "    energy = self.attn(encoder_output)\n",
        "    return torch.sum(hidden * energy, dim=2)\n",
        "\n",
        "  def concat_score(self, hidden, encoder_output):\n",
        "    energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
        "    return torch.sum(self.v * energy, dim=2)\n",
        "\n",
        "\n",
        "  def forward(self, hidden, encoder_outputs):\n",
        "    # 根据给定的方法计算注意力（能量） \n",
        "    if self.method == 'general':\n",
        "      attn_energies = self.general_score(hidden, encoder_outputs)\n",
        "    elif self.method == 'concat':\n",
        "      attn_energies = self.concat_score(hidden, encoder_outputs)\n",
        "    elif self.method == 'dot':\n",
        "      attn_energies = self.dot_score(hidden, encoder_outputs)\n",
        "    \n",
        "    # Transpose max_length and batch_size dimensions\n",
        "    attn_energies = attn_energies.t()\n",
        "    # Return the softmax normalized probability scores (with added dimension)\n",
        "    return F.softmax(attn_energies, dim=1).unsqueeze(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QK4GN4WaPM6B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 解码器GRU\n",
        "class LuongAttnDecoderRNN(nn.Module):\n",
        "  def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
        "    super(LuongAttnDecoderRNN, self).__init__()\n",
        "\n",
        "    self.attn_model = attn_model\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "    self.n_layers = n_layers\n",
        "    self.dropout = dropout\n",
        "\n",
        "    # 定义层\n",
        "    self.embedding = embedding\n",
        "    self.embedding_dropout = nn.Dropout(dropout)\n",
        "    self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
        "    self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
        "    self.out = nn.Linear(hidden_size, output_size)\n",
        "    self.attn = Attn(attn_model, hidden_size)\n",
        "  \n",
        "  \n",
        "  def forward(self, input_step, last_hidden, encoder_outputs):\n",
        "    # 注意：我们一次运行这一步（单词）\n",
        "    # 获取当前输入字的嵌入\n",
        "    embedded = self.embedding(input_step)\n",
        "    embedded = self.embedding_dropout(embedded)\n",
        "    \n",
        "    # 通过单向GRU转发\n",
        "    rnn_output, hidden = self.gru(embedded, last_hidden)\n",
        "    \n",
        "    # 从当前GRU输出计算注意力\n",
        "    attn_weights = self.attn(rnn_output, encoder_outputs)\n",
        "    \n",
        "    # 将注意力权重乘以编码器输出以获得新的“加权和”上下文向量\n",
        "    context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
        "    \n",
        "    # 使用Luong的公式五连接加权上下文向量和GRU输出\n",
        "    rnn_output = rnn_output.squeeze(0)\n",
        "    context = context.squeeze(1)\n",
        "    concat_input = torch.cat((rnn_output, context), 1)\n",
        "    concat_output = torch.tanh(self.concat(concat_input))\n",
        "    \n",
        "    # 使用Luong的公式6预测下一个单词\n",
        "    output = self.out(concat_output)\n",
        "    output = F.softmax(output, dim=1)\n",
        "    \n",
        "    # 返回输出和在最终隐藏状态\n",
        "    return output, hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STttldh-YzTX",
        "colab_type": "text"
      },
      "source": [
        "# 5.定义训练步骤\n",
        "1.   定义损失函数\n",
        "2.   定义单次训练步骤\n",
        "3.   开始迭代训练\n",
        "4.   每次迭代保存参数\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDPJKfrLRu-i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 计算平均负对数似然作为损失函数\n",
        "# 但是我们需要排除填充的0对应的tensor中的值，因为是0填充的\n",
        "\n",
        "def maskNLLLoss(inp, target, mask):\n",
        "  nTotal = mask.sum()\n",
        "  crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
        "  loss = crossEntropy.masked_select(mask).mean()\n",
        "  loss = loss.to(device)\n",
        "  return loss, nTotal.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90mgawTgWrIQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 单次训练\n",
        "\n",
        "def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding,encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=MAX_LENGTH):\n",
        "  # 零化梯度\n",
        "  encoder_optimizer.zero_grad()\n",
        "  decoder_optimizer.zero_grad()\n",
        "  \n",
        "  # 设置设备选项\n",
        "  input_variable = input_variable.to(device)\n",
        "  lengths = lengths.to(device)\n",
        "  target_variable = target_variable.to(device)\n",
        "  mask = mask.to(device)\n",
        "  \n",
        "  # 初始化变量\n",
        "  loss = 0\n",
        "  print_losses = []\n",
        "  n_totals = 0\n",
        "  \n",
        "  # 正向传递编码器\n",
        "  encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
        "  \n",
        "  # 创建初始解码器输入（从每个句子的SOS令牌开始）\n",
        "  decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
        "  decoder_input = decoder_input.to(device)\n",
        "  \n",
        "  # 将初始解码器隐藏状态设置为编码器的最终隐藏状态\n",
        "  decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "  \n",
        "  # 确定我们是否此次迭代使用`teacher forcing`\n",
        "  use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "  \n",
        "  # 通过解码器一次一步地转发一批序列\n",
        "  if use_teacher_forcing:\n",
        "    for t in range(max_target_len):\n",
        "      decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "  \n",
        "      # Teacher forcing: 下一个输入是当前的目标\n",
        "      decoder_input = target_variable[t].view(1, -1)\n",
        "\n",
        "      # 计算并累计损失\n",
        "      mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t],mask[t])\n",
        "      loss += mask_loss\n",
        "      print_losses.append(mask_loss.item() * nTotal)\n",
        "      n_totals += nTotal\n",
        "  else:\n",
        "    for t in range(max_target_len):\n",
        "      decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "  \n",
        "      # No teacher forcing: 下一个输入是解码器自己的当前输出\n",
        "      _, topi = decoder_output.topk(1)\n",
        "      decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
        "      decoder_input = decoder_input.to(device)\n",
        "\n",
        "      # 计算并累计损失\n",
        "      mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "      loss += mask_loss\n",
        "      print_losses.append(mask_loss.item() * nTotal)\n",
        "      n_totals += nTotal\n",
        "  \n",
        "  # 执行反向传播\n",
        "  loss.backward()\n",
        "  \n",
        "  # 剪辑梯度：梯度被修改到位\n",
        "  _ = torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
        "  _ = torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
        "  \n",
        "  # 调整模型权重\n",
        "  encoder_optimizer.step()\n",
        "  decoder_optimizer.step()\n",
        "  return sum(print_losses) / n_totals "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAFnFe3lYhT6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 迭代训练\n",
        "# 并为每次迭代保存模型参数\n",
        "\n",
        "def trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, \n",
        "               decoder_optimizer, embedding, encoder_n_layers, \n",
        "               decoder_n_layers, save_dir, n_iteration, batch_size, \n",
        "               print_every, save_every, clip, corpus_name, loadFilename):\n",
        "  \n",
        "  # 为每次迭代加载batches（提前将所有轮次的batch提取出来放在列表）\n",
        "  training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)]) for _ in range(n_iteration)]\n",
        "  \n",
        "  # 初始化\n",
        "  print('Initializing ...')\n",
        "  start_iteration = 1\n",
        "  print_loss = 0\n",
        "  if loadFilename:\n",
        "    start_iteration = checkpoint['iteration'] + 1\n",
        "  \n",
        "\n",
        "  # 开始循环训练\n",
        "  print(\"Training...\")\n",
        "  for iteration in range(start_iteration, n_iteration + 1):\n",
        "    training_batch = training_batches[iteration - 1]\n",
        "    \n",
        "    # 从batch中提取字段\n",
        "    input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
        "    \n",
        "    # 使用train函数对batch进行训练\n",
        "    loss = train(input_variable, lengths, target_variable, mask, \n",
        "                 max_target_len, encoder,decoder, embedding,\n",
        "                 encoder_optimizer, decoder_optimizer, \n",
        "                 batch_size, clip)\n",
        "    print_loss += loss\n",
        "    \n",
        "    # 打印进度\n",
        "    if iteration % print_every == 0:\n",
        "      print_loss_avg = print_loss / print_every\n",
        "      print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\"\n",
        "       .format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
        "      print_loss = 0\n",
        "    \n",
        "    # 每save_every轮就保存一次checkpoint\n",
        "    if (iteration % save_every == 0):\n",
        "      directory = os.path.join(\"/drive/My Drive/Model/cornell movie-dialogs corpus\", model_name, corpus_name, '{}-{}_{}'\n",
        "        .format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
        "      # 查看是否存在文件目录\n",
        "      if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "      # 保存\n",
        "      torch.save({\n",
        "      'iteration': iteration,\n",
        "      'en': encoder.state_dict(),\n",
        "      'de': decoder.state_dict(),\n",
        "      'en_opt': encoder_optimizer.state_dict(),\n",
        "      'de_opt': decoder_optimizer.state_dict(),\n",
        "      'loss': loss,\n",
        "      'voc_dict': voc.__dict__,\n",
        "      'embedding': embedding.state_dict()\n",
        "      }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hNHzlVko6hY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 贪婪解码：搜索解码器输出的decoder_output中的softmax值最大的单词\n",
        "\n",
        "class GreedySearchDecoder(nn.Module):\n",
        "  def __init__(self, encoder, decoder):\n",
        "    super(GreedySearchDecoder, self).__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "  \n",
        "  def forward(self, input_seq, input_length, max_length):\n",
        "    # 通过编码器模型转发输入\n",
        "    encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
        "    \n",
        "    # 准备编码器的最终隐藏层作为解码器的第一个隐藏输入\n",
        "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "    \n",
        "    # 使用SOS_token初始化解码器输入\n",
        "    decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
        "    \n",
        "    # 初始化张量以将解码后的单词附加到\n",
        "    all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
        "    all_scores = torch.zeros([0], device=device)\n",
        "    \n",
        "    # 一次迭代地解码一个词tokens\n",
        "    for _ in range(max_length):\n",
        "      # 正向通过解码器\n",
        "      decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "      # 获得最可能的单词标记及其softmax分数\n",
        "      decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
        "      # 记录token和分数\n",
        "      all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
        "      all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
        "      # 准备当前令牌作为下一个解码器输入（添加维度）\n",
        "      decoder_input = torch.unsqueeze(decoder_input, 0)\n",
        "    # 返回收集到的词tokens和分数\n",
        "    return all_tokens, all_scores\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJ6VIcnn4w_I",
        "colab_type": "text"
      },
      "source": [
        "#  6.定义聊天输入模块，并初始化输入"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObTP557Zw_wY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 跟机器人聊天时输入的文本，进行评估初始化\n",
        "\n",
        "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n",
        "  ### 格式化输入句子作为batch\n",
        "  # words -> indexes\n",
        "  indexes_batch = [indexesFromSentence(voc, sentence)]\n",
        "  \n",
        "  # 创建lengths张量\n",
        "  lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "  \n",
        "  # 转置batch的维度以匹配模型的期望\n",
        "  input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
        "  \n",
        "  # 使用合适的设备\n",
        "  input_batch = input_batch.to(device)\n",
        "  lengths = lengths.to(device)\n",
        "  \n",
        "  # 用searcher解码句子\n",
        "  tokens, scores = searcher(input_batch, lengths, max_length)\n",
        "  \n",
        "  # indexes -> words\n",
        "  decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
        "  return decoded_words\n",
        "\n",
        "\n",
        "def evaluateInput(encoder, decoder, searcher, voc):\n",
        "  input_sentence = ''\n",
        "  while(1):\n",
        "    try:\n",
        "      # 获取输入句子\n",
        "      input_sentence = input('> ')\n",
        "      \n",
        "      # 检查是否退出\n",
        "      if input_sentence == 'q' or input_sentence == 'quit': \n",
        "        break\n",
        "      \n",
        "      # 规范化句子\n",
        "      input_sentence = unicodeToAscii(input_sentence)\n",
        "      \n",
        "      # 评估句子\n",
        "      output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
        "      \n",
        "      # 格式化和打印回复句\n",
        "      output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
        "      print('Bot:', ' '.join(output_words))\n",
        "    except KeyError:\n",
        "      print(\"Error: Encountered unknown word.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5WrAi5gx0r0",
        "colab_type": "text"
      },
      "source": [
        "# 7.运行模型\n",
        "1.   初始化各个编码器解码器\n",
        "2.   配置从头开始训练还是从checkpoint开始\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLt8HPbnxzIT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "a40065a0-5e7b-4598-9138-d5a94d70dafd"
      },
      "source": [
        "# 配置模型\n",
        "model_name = 'cb_model'\n",
        "attn_model = 'dot'\n",
        "#attn_model = 'general'\n",
        "#attn_model = 'concat'\n",
        "hidden_size = 500\n",
        "encoder_n_layers = 2\n",
        "decoder_n_layers = 2\n",
        "dropout = 0.1\n",
        "batch_size = 64\n",
        "\n",
        "# 设置检查点以加载; 如果从头开始，则设置为None\n",
        "loadFilename = \"/drive/My Drive/Model/cornell movie-dialogs corpus/cb_model/cornell movie-dialogs corpus/2-2_500/4000_checkpoint.tar\"\n",
        "checkpoint_iter = 4000\n",
        "#loadFilename = os.path.join(save_dir, model_name, corpus_name,\n",
        "# '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size),\n",
        "# '{}_checkpoint.tar'.format(checkpoint_iter))\n",
        "\n",
        "\n",
        "# 如果提供了loadFilename（checkpoint地址），则加载模型\n",
        "if loadFilename:\n",
        "  # 如果在同一台机器上加载，则对模型进行训练\n",
        "  checkpoint = torch.load(loadFilename)\n",
        "  # If loading a model trained on GPU to CPU\n",
        "  #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
        "  encoder_sd = checkpoint['en']\n",
        "  decoder_sd = checkpoint['de']\n",
        "  encoder_optimizer_sd = checkpoint['en_opt']\n",
        "  decoder_optimizer_sd = checkpoint['de_opt']\n",
        "  embedding_sd = checkpoint['embedding']\n",
        "  voc.__dict__ = checkpoint['voc_dict']\n",
        "\n",
        "\n",
        "print('Building encoder and decoder ...')\n",
        "# 初始化词向量\n",
        "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
        "if loadFilename:\n",
        "  embedding.load_state_dict(embedding_sd)\n",
        "\n",
        "# 初始化编码器 & 解码器模型\n",
        "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
        "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, \n",
        "                              decoder_n_layers, dropout)\n",
        "if loadFilename:\n",
        "  encoder.load_state_dict(encoder_sd)\n",
        "  decoder.load_state_dict(decoder_sd)\n",
        "\n",
        "# 使用合适的设备\n",
        "encoder = encoder.to(device)\n",
        "decoder = decoder.to(device)\n",
        "print('Models built and ready to go!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building encoder and decoder ...\n",
            "Models built and ready to go!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JhY0l7DlmCT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 忽略warning\n",
        "import warnings\n",
        " \n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5ahqdJAy8N4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "4c84c811-5f3a-4acd-b628-1d6e81387c55"
      },
      "source": [
        "# 执行云训练\n",
        "\n",
        "# 配置训练/优化\n",
        "clip = 50.0\n",
        "teacher_forcing_ratio = 1.0\n",
        "learning_rate = 0.0001\n",
        "decoder_learning_ratio = 5.0\n",
        "n_iteration = 4000\n",
        "print_every = 1\n",
        "save_every = 500\n",
        "\n",
        "# 确保dropout layers在训练模型中\n",
        "encoder.train()\n",
        "decoder.train()\n",
        "\n",
        "# 初始化优化器\n",
        "print('Building optimizers ...')\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate *\n",
        "decoder_learning_ratio)\n",
        "if loadFilename:\n",
        "  encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
        "  decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
        "\n",
        "\n",
        "# 运行训练迭代\n",
        "print(\"Starting Training!\")\n",
        "trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, \n",
        "           decoder_optimizer,embedding, encoder_n_layers, decoder_n_layers, \n",
        "           save_dir, n_iteration, batch_size,print_every, save_every, \n",
        "           clip, corpus_name, loadFilename)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building optimizers ...\n",
            "Starting Training!\n",
            "Initializing ...\n",
            "Training...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0Vb5T5W0S_w",
        "colab_type": "text"
      },
      "source": [
        "# 8.和模型聊天"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLhfSwcb0RH3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        },
        "outputId": "99106208-1b08-4172-f9f8-51abe5af696c"
      },
      "source": [
        "# 将dropout layers设置为eval模式\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "\n",
        "# 初始化探索模块\n",
        "searcher = GreedySearchDecoder(encoder, decoder)\n",
        "\n",
        "# 开始聊天（取消注释并运行以下行开始）\n",
        "evaluateInput(encoder, decoder, searcher, voc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> hello\n",
            "Bot: I don't know. a lot.\n",
            "> where?\n",
            "Bot: I don't know. to be here --\n",
            "> are you ok?\n",
            "Bot: I'm okay. I'm a cab.\n",
            "> who are you\n",
            "Bot: I don't know. and Andy.\"\"\"\n",
            "> what's wrong\n",
            "Bot: What's the matter? to the picture?\n",
            "> are you kidding me\n",
            "Bot: I don't know.\n",
            "> do you know red velvet?\n",
            "Error: Encountered unknown word.\n",
            "> do you know James?\n",
            "Bot: I know. the first time. the bricks.\n",
            "> Obama\n",
            "Error: Encountered unknown word.\n",
            "> what for breakfast?\n",
            "Bot: You know what I mean.\n",
            "> ok\n",
            "Bot: You know what I mean, you're going to do?\n",
            "> shut up\n",
            "Bot: I didn't have a bad thing to be here.\n",
            "> how are you?\n",
            "Bot: I'm okay. a little far. I'm okay.\n",
            "> let's go\n",
            "Bot: I don't know. a little lady.\n",
            "> play basketball\n",
            "Bot: I don't know what you want to know.\n",
            "> do you understand?\n",
            "Bot: I don't know.\n",
            "> don't say anything\n",
            "Bot: I don't know what I want.\n",
            "> son of bitch\n",
            "Bot: I don't know where about the corner.\n",
            "> fuck\n",
            "Bot: You know what I mean, I'm afraid you were married.\n",
            "> are you married?\n",
            "Bot: I don't know.\n",
            "> quit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xn9wVuvAodU4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "0548bee3-fc45-4472-f181-24d6768986d1"
      },
      "source": [
        "b_list = [1,3,2,4]\n",
        "print([b_list,b_list])\n",
        "print(*[b_list,b_list])\n",
        "print(*zip(zip(*[b_list,b_list])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 3, 2, 4], [1, 3, 2, 4]]\n",
            "[1, 3, 2, 4] [1, 3, 2, 4]\n",
            "((1, 1),) ((3, 3),) ((2, 2),) ((4, 4),)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}